{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/28 16:37:41 WARN Utils: Your hostname, Lyna.local resolves to a loopback address: 127.0.0.1; using 10.20.83.130 instead (on interface en0)\n",
      "22/05/28 16:37:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/28 16:37:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#all restaurants:  150346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TN', 12056), ('DE', 2265), ('AZ', 9912), ('PA', 34039), ('IN', 11247), ('AB', 5573), ('NV', 7715), ('ID', 4467), ('IL', 2145), ('MT', 1), ('CA', 5203), ('MO', 10913), ('LA', 9924), ('NC', 1), ('CO', 3), ('HI', 2), ('TX', 4), ('SD', 1), ('FL', 26330), ('NJ', 8536), ('WA', 2), ('UT', 1), ('MI', 1), ('XMS', 1), ('MA', 2), ('VI', 1), ('VT', 1)]\n",
      "#selected restaurants:  8209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#selected users:  726519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30.39393925666809\n"
     ]
    }
   ],
   "source": [
    "def patch_pyspark_accumulators():\n",
    "    from inspect import getsource\n",
    "    import pyspark.accumulators as pa\n",
    "    exec(getsource(pa._start_update_server).replace(\"localhost\", \"127.0.0.1\"), pa.__dict__)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #patch_pyspark_accumulators()\n",
    "    start = time.time()\n",
    "    business_path = './yelp_dataset/business.json'\n",
    "    user_path = './yelp_dataset/user.json'\n",
    "    review_path = './yelp_dataset/review.json'\n",
    "    #sc.stop()\n",
    "    conf = SparkConf()\n",
    "    sc = SparkContext(appName='mm_exp', conf=conf)\n",
    "    \n",
    "    BusinessRDD = sc.textFile(business_path).map(lambda x: json.loads(x))\n",
    "    print('#all restaurants: ', BusinessRDD.count())\n",
    "    def countByState(data):\n",
    "        cnt = len(list(data))\n",
    "        return cnt\n",
    "    # 打印各个state的店铺数量\n",
    "    res = BusinessRDD.groupBy(lambda x: x['state']).mapValues(countByState).collect()\n",
    "    print(res)\n",
    "    # 选择店铺最多的TN，储存到selected_data中\n",
    "    BusinessRDD = BusinessRDD \\\n",
    "                        .filter(lambda x: x['state'] == 'TN') \\\n",
    "                        .filter(lambda x: x['review_count'] >= 10) \\\n",
    "                        .map(lambda x: (x['business_id'], x['stars'], x['review_count'], x['attributes'], x['categories']))\n",
    "    print('#selected restaurants: ', BusinessRDD.count())\n",
    "    BusinessStats = BusinessRDD.collect()\n",
    "    BusinessOutput = {item[0]:{'stars':item[1], 'review_count':item[2], 'attributes':item[3], 'categories':item[4]} for item in BusinessStats}\n",
    "    with open('./selected_data/business.json', 'w') as f:\n",
    "        json.dump(BusinessOutput, f)\n",
    "    \n",
    "    # 储存user中有效的信息到selected_data中\n",
    "    UserRDD = sc.textFile(user_path).map(lambda x: json.loads(x))\n",
    "    UserRDD = UserRDD \\\n",
    "                    .filter(lambda x: x['review_count'] >= 10) \\\n",
    "                    .map(lambda x: (x['user_id'], x['average_stars'], x['review_count']))\n",
    "    print('#selected users: ', UserRDD.count())\n",
    "    UserStats = UserRDD.collect()\n",
    "    UserOutput = {item[0]:{'average_stars':item[1], 'review_count':item[2]} for item in UserStats}\n",
    "    with open('./selected_data/user.json', 'w') as f:\n",
    "        json.dump(UserOutput, f)\n",
    "   \n",
    "    sc.stop()\n",
    "    end = time.time()\n",
    "    print('time: ' + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afe612ffc7409e60c6388441be73c6003a91ffa94d5a938e9b8869dfa9532966"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
